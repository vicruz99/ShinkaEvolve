# Guide to Creating New Tasks in ShinkaEvolve

To create a new task in ShinkaEvolve, you need to set up specific Python scripts for evaluation and a YAML configuration file to register the task with the system.

## 1. File Structure & Configuration

You need to create three specific files in two locations.

### Directory Setup
Create a new folder for your task inside `examples/`:
```bash
mkdir -p examples/my_new_task
```

### Required Files

| File | Location | Description |
|------|----------|-------------|
| **Initial Program** | `examples/my_new_task/initial.py` | The starting code that the LLM will evolve. Must contain the function to be optimized. |
| **Evaluator** | `examples/my_new_task/evaluate.py` | The script that validates and scores the evolved code (detailed below). |
| **Config** | `configs/task/my_new_task.yaml` | The Hydra configuration file that registers the task. |

### Configuration Template
Create `configs/task/my_new_task.yaml` with the following content:

```yaml
evaluate_function:
  _target_: examples.my_new_task.evaluate.main
  program_path: ???
  results_dir: ???

distributed_job_config:
  _target_: shinka.launch.SlurmCondaJobConfig
  modules: ["cuda/12.4", "cudnn/8.9.7", "hpcx/2.20"]
  eval_program_path: "shinka/eval_hydra.py"
  conda_env: "shinka"
  time: "00:10:00" 
  cpus: 1
  gpus: 0
  mem: "8G"

evo_config:
  task_sys_msg: |
    You are an expert developer.
    Your goal is to optimize the function `solve_problem` to maximize accuracy.
    [Add specific instructions about the problem here]
  language: "python"
  init_program_path: "examples/my_new_task/initial.py"
  job_type: "slurm_conda"

exp_name: "shinka_my_new_task"
```

---

## 2. Implementing `evaluate.py`

The core of your task logic resides in `evaluate.py`. You are essentially creating a wrapper that tells the framework **how to run**, **how to validate**, and **how to score** the code generated by the LLM.

The script must end with a call to `shinka.core.run_shinka_eval`.

### Core Components

There are 4 key components you need to implement to define a task.

#### A. The Validation Function (`validate_fn`)
This function acts as a "sanity check" or "unit test" for a single run of the evolved code. It ensures the code didn't crash, returned the right type, and obeyed hard constraints.

*   **Input**: The raw return value from the evolved function (`run_output`).
*   **Output**: A tuple `(is_valid: bool, error_message: str)`.
*   **Role**: If this returns `False`, the solution is discarded immediately as "incorrect".

```python
def validate_my_task(run_output: Any, **kwargs) -> Tuple[bool, str]:
    # 1. Check types
    if not isinstance(run_output, list):
        return False, "Output must be a list"
    
    # 2. Check constraints
    if sum(run_output) > 100:
        return False, "Sum cannot exceed 100"
        
    return True, "Valid"
```

#### B. The Argument Provider (`get_experiment_kwargs`)
This determines what arguments are passed *into* the evolved function when it is executed.

*   **Input**: `run_index` (integer, e.g., 0, 1, 2...).
*   **Output**: A dictionary of arguments unpacking into the function.
*   **Role**: Allows you to test the code against different inputs or seeds.

```python
def get_args(run_index: int) -> Dict[str, Any]:
    # Example: If the evolved function is def solve(seed):
    return {"seed": 42 + run_index}
```

#### C. The Scorer (`aggregate_metrics_fn`)
This function takes the results from *all* successful runs (if `num_runs > 1`) and calculates the final fitness score.

*   **Input**: A list of `run_output`s (only from valid runs).
*   **Output**: A dictionary containing metrics.
*   **Critical Requirement**: You **MUST** include a key called `"combined_score"`. This is the specific number the evolutionary algorithm attempts to maximize.

```python
def score_results(results: List[Any]) -> Dict[str, Any]:
    if not results:
        return {"combined_score": 0.0}

    # Example: Calculate accuracy or efficiency
    avg_score = np.mean(results)
    
    return {
        "combined_score": float(avg_score), # The value optimized by Shinka
        "details": "extra info for logging"
    }
```

#### D. The Main Execution Block
Finally, you wire everything together in a `main` function using `run_shinka_eval`.

```python
from shinka.core import run_shinka_eval
import argparse

def main(program_path: str, results_dir: str):
    metrics, correct, error_msg = run_shinka_eval(
        program_path=program_path,          # Path to the evolved code file
        results_dir=results_dir,            # Where to save artifacts
        experiment_fn_name="solve_problem", # NAME of the function inside the evolved code
        num_runs=1,                         # How many times to run the code
        get_experiment_kwargs=get_args,     # Your arg provider
        validate_fn=validate_my_task,       # Your validator
        aggregate_metrics_fn=score_results, # Your scorer
        timeout=10.0                        # (Optional) Execution timeout in seconds
    )
    
    # Print metrics for logs
    print(metrics)

if __name__ == "__main__":
    # Standard argparse boilerplate required by Hydra
    parser = argparse.ArgumentParser()
    parser.add_argument("--program_path", type=str, default="initial.py")
    parser.add_argument("--results_dir", type=str, default="results")
    args = parser.parse_args()
    main(args.program_path, args.results_dir)
```

## Summary Checklist
When creating a new task, ensure:
1.  **Function Name Match**: The `experiment_fn_name` in `evaluate.py` matches the function name in your `initial.py` (and the `sys_msg` in the yaml config).
2.  **Combined Score**: Your metrics dictionary has a `"combined_score"` key.
3.  **Robust Validation**: Your `validate_fn` catches `NaN`s, `Inf`s, and wrong types so they don't crash the aggregator.