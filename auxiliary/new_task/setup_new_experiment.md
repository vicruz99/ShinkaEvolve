# Guide to Creating New Tasks in ShinkaEvolve

This guide covers how to add new tasks to the ShinkaEvolve system. You can choose between the **Automated Workflow** (recommended for speed) or the **Manual Workflow** (for understanding the internals).

---

## ðŸš€ Automated Workflow (Recommended)

We provide utility scripts to streamline the creation and deletion of tasks. This method automatically handles file placement, configuration generation, and template replacement.

### 1. Setup Your Task Files
Navigate to `auxiliary/new_task/tasks/` and create a folder for your task (e.g., `my_new_task`). Inside this folder, you need three files:

1.  **`initial.py`**: The starting code to be evolved.
2.  **`evaluate.py`**: The logic to validate and score the code (see "Implementing evaluate.py" below for details).
3.  **`config.yaml`**: A simple configuration file defining your task parameters.

**Example `config.yaml`:**
```yaml
template:
  task: task.yaml                         # filename of the task config template ---> template/configs/task/{template.task}.yaml
  variant: variant.yaml                   # filename of the variant config template ---> template/configs/task/{template.variant}.yaml

task_name: "my_new_task"
variant_suffix: "_example"

evo_config:
  language: "python"
  task_sys_msg: |
    You are an expert developer.
    Your goal is to optimize the function `solve_problem` to maximize accuracy.
```

### 2. Deploy the Task
Run the deployment script:

```bash
# Deploys the task defined in auxiliary/new_task/tasks/my_new_task
python deploy_task.py my_new_task
```

This script will:
*   Create `examples/my_new_task/` with the files and folders inside `auxiliary/new_task/tasks/new_task/`, excluding `config.yaml`.
*   Generate `configs/task/my_new_task.yaml`.
*   Generate `configs/variant/my_new_task/my_new_task_example.yaml`.

### 3. Run the Task
Once deployed, launch the experiment using the standard launcher:

```bash
python shinka/shinka_launch task=my_new_task variant=my_new_task/variant
```

### 4. Delete a Task
To clean up a task and remove all associated files (configs and examples), run:

```bash
python delete_task.py my_new_task
```

### Configuring templates (Optional)
`my_new_task.yaml` and `my_new_task_example.yaml` are created from the configuration template files present in `auxiliary/new_task/template`. You can configure these templates or add new ones. You can select which templates to use by referencing the template file names in:

```yaml
template:
  task: task.yaml                         
  variant: variant.yaml                   
```

To add new parameters to these templates just add {new_parameter} wherever you wish, and then add the corresponding parameter in the `auxiliary/new_task/tasks/my_new_task/config.yaml`. The parameters will automatically be assigned to the right stop in the template files.


### 5. Create a venv (Optional)

If you wish 

---

## ðŸ›  Manual Workflow (Reference)

If you prefer to set up files manually, or need to debug the automated process, follow these steps.

### 1. File Structure & Configuration

You need to manually create three specific files in two locations.

#### Directory Setup
Create a new folder for your task inside `examples/`:
```bash
mkdir -p examples/my_new_task
```

#### Required Files

| File | Location | Description |
|------|----------|-------------|
| **Initial Program** | `examples/my_new_task/initial.py` | The starting code that the LLM will evolve. Must contain the function to be optimized. |
| **Evaluator** | `examples/my_new_task/evaluate.py` | The script that validates and scores the evolved code (detailed below). |

#### Configuration Template
Create `configs/task/my_new_task.yaml` with the following content:

```yaml
evaluate_function:
  _target_: examples.my_new_task.evaluate.main
  program_path: ???
  results_dir: ???

distributed_job_config:
  _target_: shinka.launch.SlurmCondaJobConfig
  modules: ["cuda/12.4", "cudnn/8.9.7", "hpcx/2.20"]
  eval_program_path: "shinka/eval_hydra.py"
  conda_env: "shinka"
  time: "00:10:00" 
  cpus: 1
  gpus: 0
  mem: "8G"

evo_config:
  task_sys_msg: |
    You are an expert developer.
    Your goal is to optimize the function `solve_problem` to maximize accuracy.
    [Add specific instructions about the problem here]
  language: "python"
  init_program_path: "examples/my_new_task/initial.py"
  job_type: "slurm_conda"

exp_name: "shinka_my_new_task"
```

---

## 2. Implementing `evaluate.py`

Whether you use the automated or manual workflow, the core of your task logic resides in `evaluate.py`. You are essentially creating a wrapper that tells the framework **how to run**, **how to validate**, and **how to score** the code generated by the LLM.

The script must end with a call to `shinka.core.run_shinka_eval`.

### Core Components

There are 4 key components you need to implement to define a task.

#### A. The Validation Function (`validate_fn`)
This function acts as a "sanity check" or "unit test" for a single run of the evolved code. It ensures the code didn't crash, returned the right type, and obeyed hard constraints.

*   **Input**: The raw return value from the evolved function (`run_output`).
*   **Output**: A tuple `(is_valid: bool, error_message: str)`.
*   **Role**: If this returns `False`, the solution is discarded immediately as "incorrect".

```python
def validate_my_task(run_output: Any, **kwargs) -> Tuple[bool, str]:
    # 1. Check types
    if not isinstance(run_output, list):
        return False, "Output must be a list"
    
    # 2. Check constraints
    if sum(run_output) > 100:
        return False, "Sum cannot exceed 100"
        
    return True, "Valid"
```

#### B. The Argument Provider (`get_experiment_kwargs`)
This determines what arguments are passed *into* the evolved function when it is executed.

*   **Input**: `run_index` (integer, e.g., 0, 1, 2...).
*   **Output**: A dictionary of arguments unpacking into the function.
*   **Role**: Allows you to test the code against different inputs or seeds.

```python
def get_args(run_index: int) -> Dict[str, Any]:
    # Example: If the evolved function is def solve(seed):
    return {"seed": 42 + run_index}
```

#### C. The Scorer (`aggregate_metrics_fn`)
This function takes the results from *all* successful runs (if `num_runs > 1`) and calculates the final fitness score.

*   **Input**: A list of `run_output`s (only from valid runs).
*   **Output**: A dictionary containing metrics.
*   **Critical Requirement**: You **MUST** include a key called `"combined_score"`. This is the specific number the evolutionary algorithm attempts to maximize.

```python
def score_results(results: List[Any]) -> Dict[str, Any]:
    if not results:
        return {"combined_score": 0.0}

    # Example: Calculate accuracy or efficiency
    avg_score = np.mean(results)
    
    return {
        "combined_score": float(avg_score), # The value optimized by Shinka
        "details": "extra info for logging"
    }
```

#### D. The Main Execution Block
Finally, you wire everything together in a `main` function using `run_shinka_eval`.

```python
from shinka.core import run_shinka_eval
import argparse

def main(program_path: str, results_dir: str):
    metrics, correct, error_msg = run_shinka_eval(
        program_path=program_path,          # Path to the evolved code file
        results_dir=results_dir,            # Where to save artifacts
        experiment_fn_name="solve_problem", # NAME of the function inside the evolved code
        num_runs=1,                         # How many times to run the code
        get_experiment_kwargs=get_args,     # Your arg provider
        validate_fn=validate_my_task,       # Your validator
        aggregate_metrics_fn=score_results, # Your scorer
        timeout=10.0                        # (Optional) Execution timeout in seconds
    )
    
    # Print metrics for logs
    print(metrics)

if __name__ == "__main__":
    # Standard argparse boilerplate required by Hydra
    parser = argparse.ArgumentParser()
    parser.add_argument("--program_path", type=str, default="initial.py")
    parser.add_argument("--results_dir", type=str, default="results")
    args = parser.parse_args()
    main(args.program_path, args.results_dir)
```

## 3. Implementing `initial.py`

This file provides the starting point for evolution. It must contain valid Python code that can be run by the evaluator.

### Critical Requirement: Evolve Blocks
ShinkaEvolve needs to know exactly which parts of the code the LLM is allowed to modify. You must mark these sections using specific comments:

*   `# EVOLVE-BLOCK-START`
*   `# EVOLVE-BLOCK-END`

Any code **outside** these blocks will remain immutable. Any code **inside** these blocks will be rewritten by the LLM during evolution.

### Template

```python
"""
Initial program for the ShinkaEvolve task.
"""
import numpy as np
import random

# Global helper functions can be defined here (outside the block if they shouldn't change).

# EVOLVE-BLOCK-START

def solve_problem(input_args=None):
    """
    The main function to be optimized.
    
    Args:
        input_args: Arguments passed from the evaluator (optional).
        
    Returns:
        The result that will be validated and scored.
    """
    # TODO: Implement a baseline solution.
    # It doesn't have to be perfect, but it MUST be valid (return the right type).
    
    # Example placeholder logic:
    result = [random.random() for _ in range(5)]
    
    return result

# EVOLVE-BLOCK-END

if __name__ == "__main__":
    # This block allows you to run the script manually to test it.
    print("Running initial program manually...")
    try:
        output = solve_problem()
        print(f"Function returned: {output}")
    except Exception as e:
        print(f"Error running function: {e}")
```

### Checklist for `initial.py`
1.  **Function Definition**: The function defined inside the block (e.g., `solve_problem`) must match the `experiment_fn_name` in `evaluate.py`.
2.  **Valid Output**: The initial implementation must return data that passes `validate_fn` in `evaluate.py`. If the initial code is invalid, the evolution cannot start.
3.  **Imports**: Ensure all necessary imports are present at the top of the file.



## Summary Checklist
When creating a new task, ensure:
1.  **Function Name Match**: The `experiment_fn_name` in `evaluate.py` matches the function name in your `initial.py` (and the `sys_msg` in the yaml config).
2.  **Combined Score**: Your metrics dictionary has a `"combined_score"` key.
3.  **Robust Validation**: Your `validate_fn` catches `NaN`s, `Inf`s, and wrong types so they don't crash the aggregator.